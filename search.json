[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML Engineer , with 7+ years of experiences",
    "section": "",
    "text": "Contact Me   Learn More \n\n\n\nServices\nEmbark on a transformative journey with my freelance services in IT and Finance Solutions. With over 6 years of experience, I specialize in Python automation, data engineering, and crafting impactful web applications and dashboards. As a seasoned freelancer, I bring a holistic approach to addressing complex challenges. My proficiency extends to designing robust ETL pipelines and implementing machine learning and AI tools to keep you at the forefront of technological innovation. Explore my portfolio for successful projects, and let’s collaborate to enhance your operational efficiency and unlock the potential of data-driven insights. Contact me today to discuss how we can tailor these advanced solutions to meet your unique business needs  \n\n\n\n\n\n\n\n\n\n\n\nArtificial Intelligence\n\n\n“I provide artificial intelligence service to business and client”\n\n\nUncover the future of business innovation through our cutting-edge Artificial Intelligence services, meticulously designed to transform operations and inspire insightful advancements.\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsulting\n\n\n“I provide consulting service to business and client”\n\n\nEnhance your business with tailored consulting services covering software engineering, data engineering, data pipeline design, AI solutions, web scraping, and lead generation. Our expertise is here to guide you towards effective solutions.\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Engineering\n\n\n“I provide data engineering solution to business and clients.”\n\n\nBoost your data capabilities with our streamlined data engineering services, featuring efficient ETL pipeline development, Apache Airflow orchestration, and scalable AWS solutions for actionable insights.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTutoring\n\n\n“I provide tutoring / mentoring service to business and client”\n\n\nEnhance your proficiency in Python, programming, software engineering, and AI through our tutoring and mentoring service. We offer tailored guidance to individuals and foster collaborative learning in group sessions, ensuring a well-rounded educational experience.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Application\n\n\n“I provide Web application service to business and client”\n\n\nStep into the future of online interaction with our Web Application services. We create user-friendly websites to make your business stand out.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping\n\n\n“I provide Web scraping service to business and client”\n\n\nEnhance your data capabilities with our specialized web scraping services, tailored for efficient data extraction, smooth automation, and real-time content discovery to elevate your decision-making processes.\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\nTestimonials\nThese are the testimonials from the awesome peoples that i have worked in the past.\n \n\n\n\n\n\n\n\n\n\n\n\nHimanshu Chaudhary\n\n\nFull Stack Engineer at Wealthfront\n\n\n“Aakash’s software expertise is unmatched. His guidance transformed our development process and boosted efficiency. Highly recommended!”\n\n\n\n\n\n\n\n\n\n\n\n\n\nSarun Luitel\n\n\nData Engineer at UNM BBER\n\n\n“Aakash’s data insights revolutionized our operations. His ETL pipeline advice was a game-changer for our data infrastructure.”\n\n\n\n\n\n\n\n\n\n\n\n\n\nTyler Ryan\n\n\nCMO at Randstad Technologies\n\n\n“Working with Aakash on AI solutions was incredible. His strategic approach and professionalism made a real impact on our business.”\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "core/solutions/1_Artificial Intelligence/artificial_intelligence.html",
    "href": "core/solutions/1_Artificial Intelligence/artificial_intelligence.html",
    "title": "Artificial Intelligence",
    "section": "",
    "text": "Uncover the future of business innovation through our cutting-edge Artificial Intelligence services, meticulously designed to transform operations and inspire insightful advancements.\n\n\nExperience the pinnacle of AI solutions tailored precisely to your unique business demands, leveraging a suite of sophisticated tools and technologies.\n\n\n\nEmbark on a journey of comprehensive AI capabilities, including: - Crafting advanced machine learning models. - Elevating communication with natural language processing. - Mastering intricate data analysis through deep learning applications. - Optimizing processes with streamlined, data-driven automation.\n\n\n\nHarness the power of industry-standard tools and technologies within our AI solutions: - TensorFlow and PyTorch for robust machine learning. - NLP prowess with NLTK and spaCy. - Cutting-edge frameworks for deep learning. - Seamless integration through automation platforms.\n\n\n\nWitness the tangible impact of our recent AI endeavors: - Precision in predictive analytics with sophisticated machine learning models. - Elevated communication and understanding through NLP solutions. - Advanced image and pattern recognition with deep learning applications.\n\n\n\nWhether you have specific AI requirements or aspire to innovate, let’s connect and explore how our end-to-end AI expertise can elevate and transform your business operations."
  },
  {
    "objectID": "core/solutions/1_Artificial Intelligence/artificial_intelligence.html#tailored-ai-excellence",
    "href": "core/solutions/1_Artificial Intelligence/artificial_intelligence.html#tailored-ai-excellence",
    "title": "Artificial Intelligence",
    "section": "",
    "text": "Experience the pinnacle of AI solutions tailored precisely to your unique business demands, leveraging a suite of sophisticated tools and technologies."
  },
  {
    "objectID": "core/solutions/1_Artificial Intelligence/artificial_intelligence.html#end-to-end-ai-mastery",
    "href": "core/solutions/1_Artificial Intelligence/artificial_intelligence.html#end-to-end-ai-mastery",
    "title": "Artificial Intelligence",
    "section": "",
    "text": "Embark on a journey of comprehensive AI capabilities, including: - Crafting advanced machine learning models. - Elevating communication with natural language processing. - Mastering intricate data analysis through deep learning applications. - Optimizing processes with streamlined, data-driven automation."
  },
  {
    "objectID": "core/solutions/1_Artificial Intelligence/artificial_intelligence.html#tools-and-technologies",
    "href": "core/solutions/1_Artificial Intelligence/artificial_intelligence.html#tools-and-technologies",
    "title": "Artificial Intelligence",
    "section": "",
    "text": "Harness the power of industry-standard tools and technologies within our AI solutions: - TensorFlow and PyTorch for robust machine learning. - NLP prowess with NLTK and spaCy. - Cutting-edge frameworks for deep learning. - Seamless integration through automation platforms."
  },
  {
    "objectID": "core/solutions/1_Artificial Intelligence/artificial_intelligence.html#recent-ai-triumphs",
    "href": "core/solutions/1_Artificial Intelligence/artificial_intelligence.html#recent-ai-triumphs",
    "title": "Artificial Intelligence",
    "section": "",
    "text": "Witness the tangible impact of our recent AI endeavors: - Precision in predictive analytics with sophisticated machine learning models. - Elevated communication and understanding through NLP solutions. - Advanced image and pattern recognition with deep learning applications."
  },
  {
    "objectID": "core/solutions/1_Artificial Intelligence/artificial_intelligence.html#propel-your-operations-forward",
    "href": "core/solutions/1_Artificial Intelligence/artificial_intelligence.html#propel-your-operations-forward",
    "title": "Artificial Intelligence",
    "section": "",
    "text": "Whether you have specific AI requirements or aspire to innovate, let’s connect and explore how our end-to-end AI expertise can elevate and transform your business operations."
  },
  {
    "objectID": "core/solutions/2_Data Engineering/data-engineering.html",
    "href": "core/solutions/2_Data Engineering/data-engineering.html",
    "title": "Data Engineering",
    "section": "",
    "text": "Discover our versatile data engineering services designed to optimize your data infrastructure and drive actionable insights across your organization.\n\n\nWe specialize in crafting tailored data engineering solutions to meet your specific business needs, leveraging a variety of tools and technologies.\n\n\n\n\nEfficient ETL (Extract, Transform, Load) pipeline development.\nStreamlining data workflows for improved efficiency.\nData cleansing, transformation, and enrichment processes.\nIntegration of diverse data sources for comprehensive analysis.\nImplementing scalable and robust data architectures.\n\n\n\n\nOur data engineering solutions make use of industry-standard tools and technologies, ensuring reliability and performance in handling your data:\n\nApache Airflow for orchestration and scheduling.\nApache Spark for distributed data processing.\nApache Hadoop for large-scale data storage and processing.\nSQL-based databases (e.g., PostgreSQL, MySQL).\nNoSQL databases (e.g., MongoDB, Cassandra).\nCloud-based data services, particularly AWS for scalable and secure solutions.\n\n\n\n\nExplore some of our recent successes in the realm of data engineering:\n\nDevelopment of a scalable ETL pipeline using Apache Airflow for real-time data processing.\nImplementation of data cleansing and enrichment processes for improved data quality.\nIntegration of various data sources into AWS for unified business intelligence.\n\n\n\n\nIf you have data engineering requirements or are looking to optimize your data processes, let’s connect and explore how our expertise can elevate your data infrastructure."
  },
  {
    "objectID": "core/solutions/2_Data Engineering/data-engineering.html#custom-data-engineering-solutions",
    "href": "core/solutions/2_Data Engineering/data-engineering.html#custom-data-engineering-solutions",
    "title": "Data Engineering",
    "section": "",
    "text": "We specialize in crafting tailored data engineering solutions to meet your specific business needs, leveraging a variety of tools and technologies."
  },
  {
    "objectID": "core/solutions/2_Data Engineering/data-engineering.html#data-engineering-capabilities",
    "href": "core/solutions/2_Data Engineering/data-engineering.html#data-engineering-capabilities",
    "title": "Data Engineering",
    "section": "",
    "text": "Efficient ETL (Extract, Transform, Load) pipeline development.\nStreamlining data workflows for improved efficiency.\nData cleansing, transformation, and enrichment processes.\nIntegration of diverse data sources for comprehensive analysis.\nImplementing scalable and robust data architectures."
  },
  {
    "objectID": "core/solutions/2_Data Engineering/data-engineering.html#tools-and-technologies",
    "href": "core/solutions/2_Data Engineering/data-engineering.html#tools-and-technologies",
    "title": "Data Engineering",
    "section": "",
    "text": "Our data engineering solutions make use of industry-standard tools and technologies, ensuring reliability and performance in handling your data:\n\nApache Airflow for orchestration and scheduling.\nApache Spark for distributed data processing.\nApache Hadoop for large-scale data storage and processing.\nSQL-based databases (e.g., PostgreSQL, MySQL).\nNoSQL databases (e.g., MongoDB, Cassandra).\nCloud-based data services, particularly AWS for scalable and secure solutions."
  },
  {
    "objectID": "core/solutions/2_Data Engineering/data-engineering.html#recent-data-engineering-projects",
    "href": "core/solutions/2_Data Engineering/data-engineering.html#recent-data-engineering-projects",
    "title": "Data Engineering",
    "section": "",
    "text": "Explore some of our recent successes in the realm of data engineering:\n\nDevelopment of a scalable ETL pipeline using Apache Airflow for real-time data processing.\nImplementation of data cleansing and enrichment processes for improved data quality.\nIntegration of various data sources into AWS for unified business intelligence."
  },
  {
    "objectID": "core/solutions/2_Data Engineering/data-engineering.html#lets-enhance-your-data-infrastructure",
    "href": "core/solutions/2_Data Engineering/data-engineering.html#lets-enhance-your-data-infrastructure",
    "title": "Data Engineering",
    "section": "",
    "text": "If you have data engineering requirements or are looking to optimize your data processes, let’s connect and explore how our expertise can elevate your data infrastructure."
  },
  {
    "objectID": "core/solutions/6_Consulting/consulting.html",
    "href": "core/solutions/6_Consulting/consulting.html",
    "title": "Consulting",
    "section": "",
    "text": "Enhance your business with tailored consulting services covering software engineering, data engineering, data pipeline design, AI solutions, web scraping, and lead generation. Our expertise is here to guide you towards effective solutions.\n\n\nReceive personalized advice and strategies to optimize your software development processes, ensuring efficient and robust solutions.\n\n\n\nUnlock the potential of your data infrastructure with consulting in data engineering, including ETL pipelines, database design, and data architecture.\n\n\n\nGet insights into designing effective data pipelines that streamline the flow of information, ensuring seamless data processing.\n\n\n\nExplore the possibilities of AI for your business with expert consultation, covering machine learning models, natural language processing, and AI integration.\n\n\n\nGain an edge in data extraction and automation with consulting on web scraping techniques, tools, and best practices.\n\n\n\nTransform your business with lead generation strategies, leveraging data-driven approaches for targeted and effective outreach.\n\n\n\nWe provide: - Personalized consultations tailored to your business needs. - Practical strategies to implement efficient solutions. - Expert guidance in navigating complex technological challenges.\n\n\n\nDiscover how our consulting services have made a difference: - Optimizing software development processes. - Enhancing data infrastructure for improved insights. - Designing effective data pipelines for streamlined operations. - Implementing successful AI solutions for various industries. - Crafting web scraping strategies for data-driven decision-making. - Boosting lead generation efforts with targeted approaches.\n\n\n\nWhether you’re seeking advice on software engineering, data solutions, or strategic insights for AI, web scraping, and lead generation, let’s connect. Our consulting services are here to guide you toward effective and impactful solutions."
  },
  {
    "objectID": "core/solutions/6_Consulting/consulting.html#software-engineering-consultation",
    "href": "core/solutions/6_Consulting/consulting.html#software-engineering-consultation",
    "title": "Consulting",
    "section": "",
    "text": "Receive personalized advice and strategies to optimize your software development processes, ensuring efficient and robust solutions."
  },
  {
    "objectID": "core/solutions/6_Consulting/consulting.html#data-engineering-expertise",
    "href": "core/solutions/6_Consulting/consulting.html#data-engineering-expertise",
    "title": "Consulting",
    "section": "",
    "text": "Unlock the potential of your data infrastructure with consulting in data engineering, including ETL pipelines, database design, and data architecture."
  },
  {
    "objectID": "core/solutions/6_Consulting/consulting.html#data-pipeline-design",
    "href": "core/solutions/6_Consulting/consulting.html#data-pipeline-design",
    "title": "Consulting",
    "section": "",
    "text": "Get insights into designing effective data pipelines that streamline the flow of information, ensuring seamless data processing."
  },
  {
    "objectID": "core/solutions/6_Consulting/consulting.html#ai-solutions-consulting",
    "href": "core/solutions/6_Consulting/consulting.html#ai-solutions-consulting",
    "title": "Consulting",
    "section": "",
    "text": "Explore the possibilities of AI for your business with expert consultation, covering machine learning models, natural language processing, and AI integration."
  },
  {
    "objectID": "core/solutions/6_Consulting/consulting.html#web-scraping-strategies",
    "href": "core/solutions/6_Consulting/consulting.html#web-scraping-strategies",
    "title": "Consulting",
    "section": "",
    "text": "Gain an edge in data extraction and automation with consulting on web scraping techniques, tools, and best practices."
  },
  {
    "objectID": "core/solutions/6_Consulting/consulting.html#lead-generation-insights",
    "href": "core/solutions/6_Consulting/consulting.html#lead-generation-insights",
    "title": "Consulting",
    "section": "",
    "text": "Transform your business with lead generation strategies, leveraging data-driven approaches for targeted and effective outreach."
  },
  {
    "objectID": "core/solutions/6_Consulting/consulting.html#our-approach",
    "href": "core/solutions/6_Consulting/consulting.html#our-approach",
    "title": "Consulting",
    "section": "",
    "text": "We provide: - Personalized consultations tailored to your business needs. - Practical strategies to implement efficient solutions. - Expert guidance in navigating complex technological challenges."
  },
  {
    "objectID": "core/solutions/6_Consulting/consulting.html#recent-success-stories",
    "href": "core/solutions/6_Consulting/consulting.html#recent-success-stories",
    "title": "Consulting",
    "section": "",
    "text": "Discover how our consulting services have made a difference: - Optimizing software development processes. - Enhancing data infrastructure for improved insights. - Designing effective data pipelines for streamlined operations. - Implementing successful AI solutions for various industries. - Crafting web scraping strategies for data-driven decision-making. - Boosting lead generation efforts with targeted approaches."
  },
  {
    "objectID": "core/solutions/6_Consulting/consulting.html#elevate-your-business-strategies",
    "href": "core/solutions/6_Consulting/consulting.html#elevate-your-business-strategies",
    "title": "Consulting",
    "section": "",
    "text": "Whether you’re seeking advice on software engineering, data solutions, or strategic insights for AI, web scraping, and lead generation, let’s connect. Our consulting services are here to guide you toward effective and impactful solutions."
  },
  {
    "objectID": "core/testimonials/3_Himanshu_Chaudhary/Himanshu_Chaudhary.html",
    "href": "core/testimonials/3_Himanshu_Chaudhary/Himanshu_Chaudhary.html",
    "title": "Himanshu Chaudhary",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "core/testimonials/2_Sarun_Luitel/sarun_luitel.html",
    "href": "core/testimonials/2_Sarun_Luitel/sarun_luitel.html",
    "title": "Sarun Luitel",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "core/blogs/running_llm_locally/running_llm_locally.html",
    "href": "core/blogs/running_llm_locally/running_llm_locally.html",
    "title": "Running LLM locally",
    "section": "",
    "text": "LLM and AI (Generated by Imagen3)\n\n\n\nInstallation\nOn your terminal run the following command to install ollama module.\npip install ollama\n\nPull and run the model of you choice. For this tutorial we are running Gemma2 2B model, since it is small and powerful\nollama pull gemma2:2b\nInstall Langchain-ollama module\npip install langchain-ollama\n\n\nImport Model\nFor this experiment we will loading gemma2:2b using ollama and langchain model. The temperature is set to 0. Temperature near to 0 makes llm output more deterministic. For application executing certain task the temperature should be set near to 0. But for task like generating poem or movie scricpt the temperature should be set near to 1. The temperature near to 1 makes llm more creative and add ramdomness element to the process.\n\nfrom langchain_ollama.llms import OllamaLLM\nfrom langchain_core.prompts import ChatPromptTemplate\nimport json\nfrom pprint import pprint\n\nllm = OllamaLLM(model=\"gemma2:2b\",\n                temperature = 0)\n\n\n\nLets build LLM agent to translate one language to another.\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"\"\"\n            You are a helpful assistant that translates {input_language} to {output_language}. \n            Provides output in json format as language as key and outpus as value\n            \"\"\",\n        ),\n        (\"human\", \"{input}\"),\n    ]\n)\n\nchain = prompt | llm\n\nai_message = chain.invoke(\n    {\n        \"input_language\": \"English\",\n        \"output_language\": \"Spanish, German, Korean\",\n        \"input\": \"I love grilled chicken\",\n    }\n)\n\npprint(ai_message)\n\n('```json\\n'\n '{\\n'\n '  \"English\": \"I love grilled chicken\",\\n'\n '  \"Spanish\": \"¡Me encanta el pollo a la parrilla!\",\\n'\n '  \"German\": \"Ich liebe gegrillte Hähnchen!\",\\n'\n '  \"Korean\": \"닭갈비를 좋아해요!\" \\n'\n '}\\n'\n '```')\n\n\nThe llm model response is in string format. Now, lets load the string response as json object.\n\n# Replace and assign back to original content\nai_message = ai_message.replace(\"```json\", \"\")\nai_message = ai_message.replace(\"```\", \"\")\n\n# Don't forget to convert to JSON as it is a string right now:\njson_result = json.loads(ai_message)\npprint(json_result)\n\n{'English': 'I love grilled chicken',\n 'German': 'Ich liebe gegrillte Hähnchen!',\n 'Korean': '닭갈비를 좋아해요!',\n 'Spanish': '¡Me encanta el pollo a la parrilla!'}\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "core/blogs/mlops/mlops.html",
    "href": "core/blogs/mlops/mlops.html",
    "title": "ML OPS Ideas",
    "section": "",
    "text": "MLOPS (Generated by Imagen 3)"
  },
  {
    "objectID": "core/blogs/mlops/mlops.html#level-1-implementation",
    "href": "core/blogs/mlops/mlops.html#level-1-implementation",
    "title": "ML OPS Ideas",
    "section": "Level 1 implementation",
    "text": "Level 1 implementation\n model table \n\n\n\nid\nmodel name\nserving version\n\n\n\n\n1\nnlp\n1.0\n\n\n2\nrandomforest\n1.2\n\n\n3\nRNN\n1.0\n\n\n\nmodel artifact table \n\n\n\nid\nmodel_id\nversion\nimage\nmetrics\n\n\n\n\na1\n1\n1.0\n01010\n——-\n\n\na2\n2\n1.0\n10111\n——-\n\n\na3\n2\n1.1\n00001\n——-\n\n\na4\n2\n1.2\n00010\n——-\n\n\na5\n3\n1.0\n00100\n——-\n\n\na6\n3\n1.1\n10011\n——-"
  },
  {
    "objectID": "core/blogs/mlops/mlops.html#level-2",
    "href": "core/blogs/mlops/mlops.html#level-2",
    "title": "ML OPS Ideas",
    "section": "Level 2:",
    "text": "Level 2:\nOpen source tools like mlflow, comes with built in model tracking sercive. One can host the model registry service by setting up custom tracking server for registering the model. Moreover, mlflow also has built in UI for model tracking.\nAWS Sagemaker also provides model registry service, which is easy to setup and use."
  },
  {
    "objectID": "core/blogs/mlops/mlops.html#level1-batch-processig-jobs",
    "href": "core/blogs/mlops/mlops.html#level1-batch-processig-jobs",
    "title": "ML OPS Ideas",
    "section": "Level1: Batch processig jobs",
    "text": "Level1: Batch processig jobs\nBatch processing python scripts can be scheduled to checkout the model from model registry and make predictions by feeding the data to the models. For, Tabaleu reporting , the python scripts can be scheduled to connect to Tabaleu server and write predictions. The structure of the batch scripts looks like below:\n\nExtract the data to make predictions\nTransform the data using feature engineering pipeline script\nCheckout the serving model from model registery\nMake predictions\nTransform the prediction to make it ready for datbase.\nUpdate the database with pediction value\nProvide the feedback and store the new features to feature registry"
  },
  {
    "objectID": "core/blogs/mlops/mlops.html#level-2-api",
    "href": "core/blogs/mlops/mlops.html#level-2-api",
    "title": "ML OPS Ideas",
    "section": "Level 2: API",
    "text": "Level 2: API\nThe model can be packaged and served as API(Model as a Service). User can call API for and post their data for prediction. The API end point will trigger the pipeline make predictions. The batch processing job could still call the API to make predictions. API could be integrated to databases, API, microservices, tabaleu, excel. APIs can be served as server side templating(JinJa, HTML, markdowns) or whole fornt end(React, Anguler, Vue)\n\nOptions:\n\nFAST API: Rapid Development, Asyncronys design, auto documentation and data validation through pydantic data models, server side templating compatible.\nFLASK: Has more support community and is in market for longer time, server side templating compatible."
  },
  {
    "objectID": "core/blogs/mlops/mlops.html#level-3-adding-message-queueing-system-to-the-api-backed-or-batch-processing-workflow.",
    "href": "core/blogs/mlops/mlops.html#level-3-adding-message-queueing-system-to-the-api-backed-or-batch-processing-workflow.",
    "title": "ML OPS Ideas",
    "section": "Level 3: Adding message queueing system to the API backed or batch processing workflow.",
    "text": "Level 3: Adding message queueing system to the API backed or batch processing workflow.\nAs vloume and velocity of the data grows, the API could not handle the incoming request. This problem could be solved by implementing Message Broker system, when the incoming requests api are published to message queues and the model will make prediction by consuming the data from message queues and publish the data to the Message queue. This way the large datasets can be consumed and published/stream.\n\nOptions:\n\nKakfa\nAmazon SQS"
  },
  {
    "objectID": "core/blogs/Linear-regression-using-DNN/Linear-regression-using-DNN.html",
    "href": "core/blogs/Linear-regression-using-DNN/Linear-regression-using-DNN.html",
    "title": "Linear Regression using DNN",
    "section": "",
    "text": "from sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nhousing = fetch_california_housing()\nhousing\n\n\n{'data': array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n           37.88      , -122.23      ],\n        [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n           37.86      , -122.22      ],\n        [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n           37.85      , -122.24      ],\n        ...,\n        [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n           39.43      , -121.22      ],\n        [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n           39.43      , -121.32      ],\n        [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n           39.37      , -121.24      ]]),\n 'target': array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894]),\n 'frame': None,\n 'target_names': ['MedHouseVal'],\n 'feature_names': ['MedInc',\n  'HouseAge',\n  'AveRooms',\n  'AveBedrms',\n  'Population',\n  'AveOccup',\n  'Latitude',\n  'Longitude'],\n 'DESCR': '.. _california_housing_dataset:\\n\\nCalifornia Housing dataset\\n--------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 20640\\n\\n:Number of Attributes: 8 numeric, predictive attributes and the target\\n\\n:Attribute Information:\\n    - MedInc        median income in block group\\n    - HouseAge      median house age in block group\\n    - AveRooms      average number of rooms per household\\n    - AveBedrms     average number of bedrooms per household\\n    - Population    block group population\\n    - AveOccup      average number of household members\\n    - Latitude      block group latitude\\n    - Longitude     block group longitude\\n\\n:Missing Attribute Values: None\\n\\nThis dataset was obtained from the StatLib repository.\\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\\n\\nThe target variable is the median house value for California districts,\\nexpressed in hundreds of thousands of dollars ($100,000).\\n\\nThis dataset was derived from the 1990 U.S. census, using one row per census\\nblock group. A block group is the smallest geographical unit for which the U.S.\\nCensus Bureau publishes sample data (a block group typically has a population\\nof 600 to 3,000 people).\\n\\nA household is a group of people residing within a home. Since the average\\nnumber of rooms and bedrooms in this dataset are provided per household, these\\ncolumns may take surprisingly large values for block groups with few households\\nand many empty houses, such as vacation resorts.\\n\\nIt can be downloaded/loaded using the\\n:func:`sklearn.datasets.fetch_california_housing` function.\\n\\n.. topic:: References\\n\\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\n      Statistics and Probability Letters, 33 (1997) 291-297\\n'}\n\n\n\nx_train_data, x_test, y_train_data, y_test = train_test_split(housing.data, housing.target)\nx_train, x_valid, y_train, y_valid = train_test_split(x_train_data, y_train_data)\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_valid = scaler.fit_transform(x_valid)\nx_test = scaler.fit_transform(x_test)"
  },
  {
    "objectID": "core/blogs/Linear-regression-using-DNN/Linear-regression-using-DNN.html#linear-regression-of-california-housing-dataset-using-deep-neural-networks",
    "href": "core/blogs/Linear-regression-using-DNN/Linear-regression-using-DNN.html#linear-regression-of-california-housing-dataset-using-deep-neural-networks",
    "title": "Linear Regression using DNN",
    "section": "",
    "text": "from sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nhousing = fetch_california_housing()\nhousing\n\n\n{'data': array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n           37.88      , -122.23      ],\n        [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n           37.86      , -122.22      ],\n        [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n           37.85      , -122.24      ],\n        ...,\n        [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n           39.43      , -121.22      ],\n        [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n           39.43      , -121.32      ],\n        [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n           39.37      , -121.24      ]]),\n 'target': array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894]),\n 'frame': None,\n 'target_names': ['MedHouseVal'],\n 'feature_names': ['MedInc',\n  'HouseAge',\n  'AveRooms',\n  'AveBedrms',\n  'Population',\n  'AveOccup',\n  'Latitude',\n  'Longitude'],\n 'DESCR': '.. _california_housing_dataset:\\n\\nCalifornia Housing dataset\\n--------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 20640\\n\\n:Number of Attributes: 8 numeric, predictive attributes and the target\\n\\n:Attribute Information:\\n    - MedInc        median income in block group\\n    - HouseAge      median house age in block group\\n    - AveRooms      average number of rooms per household\\n    - AveBedrms     average number of bedrooms per household\\n    - Population    block group population\\n    - AveOccup      average number of household members\\n    - Latitude      block group latitude\\n    - Longitude     block group longitude\\n\\n:Missing Attribute Values: None\\n\\nThis dataset was obtained from the StatLib repository.\\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\\n\\nThe target variable is the median house value for California districts,\\nexpressed in hundreds of thousands of dollars ($100,000).\\n\\nThis dataset was derived from the 1990 U.S. census, using one row per census\\nblock group. A block group is the smallest geographical unit for which the U.S.\\nCensus Bureau publishes sample data (a block group typically has a population\\nof 600 to 3,000 people).\\n\\nA household is a group of people residing within a home. Since the average\\nnumber of rooms and bedrooms in this dataset are provided per household, these\\ncolumns may take surprisingly large values for block groups with few households\\nand many empty houses, such as vacation resorts.\\n\\nIt can be downloaded/loaded using the\\n:func:`sklearn.datasets.fetch_california_housing` function.\\n\\n.. topic:: References\\n\\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\n      Statistics and Probability Letters, 33 (1997) 291-297\\n'}\n\n\n\nx_train_data, x_test, y_train_data, y_test = train_test_split(housing.data, housing.target)\nx_train, x_valid, y_train, y_valid = train_test_split(x_train_data, y_train_data)\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_valid = scaler.fit_transform(x_valid)\nx_test = scaler.fit_transform(x_test)"
  },
  {
    "objectID": "core/blogs/Linear-regression-using-DNN/Linear-regression-using-DNN.html#training-the-model-using-sequencital-api",
    "href": "core/blogs/Linear-regression-using-DNN/Linear-regression-using-DNN.html#training-the-model-using-sequencital-api",
    "title": "Linear Regression using DNN",
    "section": "Training the model using Sequencital API",
    "text": "Training the model using Sequencital API\n\nimport keras\n\n\ninput_layer = keras.layers.Dense(30, activation='relu', input_shape = x_train.shape[1:])\noutput_layer = keras.layers.Dense(1)\n\nmodel = keras.models.Sequential([input_layer, output_layer])\nmodel.compile(loss='mean_squared_error', optimizer='sgd')\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 30)                270       \n                                                                 \n dense_1 (Dense)             (None, 1)                 31        \n                                                                 \n=================================================================\nTotal params: 301 (1.18 KB)\nTrainable params: 301 (1.18 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\nfrom keras.utils import plot_model\nplot_model(\n    model,\n    show_shapes=True,\n    show_dtype=False,\n    show_layer_names=False,\n    rankdir=\"LR\",\n    expand_nested=False,\n    show_layer_activations= True,\n    dpi=96,\n)\n\n\n\n\n\n\n\n\n\n\nhistory = model.fit(x_train, y_train, epochs=10, validation_data = (x_valid, y_valid)) \n\nEpoch 1/10\n363/363 [==============================] - 0s 534us/step - loss: 0.7398 - val_loss: 0.4427\nEpoch 2/10\n363/363 [==============================] - 0s 496us/step - loss: 0.5281 - val_loss: 0.4122\nEpoch 3/10\n363/363 [==============================] - 0s 346us/step - loss: 0.4744 - val_loss: 0.5118\nEpoch 4/10\n363/363 [==============================] - 0s 337us/step - loss: 0.4596 - val_loss: 0.4985\nEpoch 5/10\n363/363 [==============================] - 0s 345us/step - loss: 0.4481 - val_loss: 0.5587\nEpoch 6/10\n363/363 [==============================] - 0s 370us/step - loss: 0.4368 - val_loss: 0.7226\nEpoch 7/10\n363/363 [==============================] - 0s 364us/step - loss: 0.4358 - val_loss: 0.7338\nEpoch 8/10\n363/363 [==============================] - 0s 356us/step - loss: 0.4266 - val_loss: 0.9216\nEpoch 9/10\n363/363 [==============================] - 0s 365us/step - loss: 0.4444 - val_loss: 0.8432\nEpoch 10/10\n363/363 [==============================] - 0s 376us/step - loss: 0.4182 - val_loss: 1.0728\n\n\n\nmodel.evaluate(x_test,y_test)\n\n162/162 [==============================] - 0s 241us/step - loss: 0.3810\n\n\n0.3810201585292816\n\n\n\nx_1 = x_test[:3]\ny_pred = model.predict(x_1)\nprint(y_pred)\n\n1/1 [==============================] - 0s 38ms/step\n[[3.08389  ]\n [1.2611595]\n [0.9729465]]"
  },
  {
    "objectID": "core/blogs/Linear-regression-using-DNN/Linear-regression-using-DNN.html#building-complex-models-ushing-functional-api-non-sequential",
    "href": "core/blogs/Linear-regression-using-DNN/Linear-regression-using-DNN.html#building-complex-models-ushing-functional-api-non-sequential",
    "title": "Linear Regression using DNN",
    "section": "Building complex models ushing Functional API (non sequential)",
    "text": "Building complex models ushing Functional API (non sequential)\n\ninput_ = keras.layers.Input(shape = x_train.shape[1:])\nhidden_layer_1 = keras.layers.Dense(30, activation = 'relu')(input_)\nhidden_layer_2 = keras.layers.Dense(30, activation='relu') (hidden_layer_1)\nconcat_layer = keras.layers.Concatenate()([input_, hidden_layer_2])\noutput_layer = keras.layers.Dense(1)(concat_layer)\n\nmodel2 = keras.Model(inputs = [input_], outputs = [output_layer])\nmodel2.summary()\n\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_1 (InputLayer)        [(None, 8)]                  0         []                            \n                                                                                                  \n dense_2 (Dense)             (None, 30)                   270       ['input_1[0][0]']             \n                                                                                                  \n dense_3 (Dense)             (None, 30)                   930       ['dense_2[0][0]']             \n                                                                                                  \n concatenate (Concatenate)   (None, 38)                   0         ['input_1[0][0]',             \n                                                                     'dense_3[0][0]']             \n                                                                                                  \n dense_4 (Dense)             (None, 1)                    39        ['concatenate[0][0]']         \n                                                                                                  \n==================================================================================================\nTotal params: 1239 (4.84 KB)\nTrainable params: 1239 (4.84 KB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n\n\n\nplot_model(\n    model2,\n    show_shapes=True,\n    show_dtype=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    show_layer_activations= True,\n    dpi=96,\n)"
  },
  {
    "objectID": "core/blogs/image_classification/image_classification.html",
    "href": "core/blogs/image_classification/image_classification.html",
    "title": "Image classification",
    "section": "",
    "text": "Code\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(tf.__version__)\n\nfashion_data = tf.keras.datasets.fashion_mnist\n(train_data, train_label) , (test_data, test_label) = fashion_data.load_data()\n\n\n2.15.0"
  },
  {
    "objectID": "core/blogs/image_classification/image_classification.html#import-the-fashion-mnist-dataset",
    "href": "core/blogs/image_classification/image_classification.html#import-the-fashion-mnist-dataset",
    "title": "Image classification",
    "section": "",
    "text": "Code\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(tf.__version__)\n\nfashion_data = tf.keras.datasets.fashion_mnist\n(train_data, train_label) , (test_data, test_label) = fashion_data.load_data()\n\n\n2.15.0"
  },
  {
    "objectID": "core/blogs/image_classification/image_classification.html#preprocessing-the-data",
    "href": "core/blogs/image_classification/image_classification.html#preprocessing-the-data",
    "title": "Image classification",
    "section": "Preprocessing the data",
    "text": "Preprocessing the data\n\n\nCode\ntrain_data.shape\n\n\n(60000, 28, 28)\n\n\n\n\nCode\ntest_label.shape\n\n\n(10000,)\n\n\n\n\nCode\nplt.figure()\nplt.imshow(train_data[0])\nplt.colorbar()\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nx_valid, x_train = train_data[:5000] / 255.0, train_data[5000:]/255.0\ny_valid, y_train = train_label[:5000], train_label[5000:]\n\n\n\n\nCode\nclass_map = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n\n\n\nCode\nplt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5, i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.xlabel(class_map[train_label[i]])\n    plt.imshow(train_data[i])\nplt.show()"
  },
  {
    "objectID": "core/blogs/image_classification/image_classification.html#building-the-model-using-neural-network",
    "href": "core/blogs/image_classification/image_classification.html#building-the-model-using-neural-network",
    "title": "Image classification",
    "section": "Building the model using neural network",
    "text": "Building the model using neural network\nSequential Model is the simplest form of Model that keras library provides. It is composed of a single stack of layer connected sequentially. Also, known as sequential API. The first layer in the model is is Flatten model ,which simply flattens the 2D image array to 1D. Then, the Dense layer with 300 neurons which uses Relu as an activation function is stacked. Then, there is second Dense layer with 100 neurons , with same Relu as activation function. The final layer in the model stack is output layuer, which is also a Dense layer with 10 neurons (one per class) and it uses softmax function for multi-class classification.\n\n\nCode\ninput_layer = tf.keras.layers.Flatten(input_shape=(28,28))\ndense_layer1 = tf.keras.layers.Dense(300, activation='relu')\ndense_layer2 = tf.keras.layers.Dense(100, activation='relu')\noutput_layer = tf.keras.layers.Dense(10, activation = 'softmax')\n\nmodel = tf.keras.Sequential([input_layer,dense_layer1, dense_layer2 ,output_layer])\noptimizer = 'adam'\nloss_function = tf.keras.losses.SparseCategoricalCrossentropy()\nmetrics = ['accuracy']\n\nmodel.summary()\n\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten (Flatten)           (None, 784)               0         \n                                                                 \n dense (Dense)               (None, 300)               235500    \n                                                                 \n dense_1 (Dense)             (None, 100)               30100     \n                                                                 \n dense_2 (Dense)             (None, 10)                1010      \n                                                                 \n=================================================================\nTotal params: 266610 (1.02 MB)\nTrainable params: 266610 (1.02 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________"
  },
  {
    "objectID": "core/blogs/image_classification/image_classification.html#training-the-model",
    "href": "core/blogs/image_classification/image_classification.html#training-the-model",
    "title": "Image classification",
    "section": "Training the model",
    "text": "Training the model\n\n\nCode\nmodel.compile(optimizer=optimizer, loss= loss_function, metrics=metrics )\nhistory = model.fit(x_train,y_train, epochs=10, validation_data = (x_valid, y_valid))\n\n\nEpoch 1/10\n1719/1719 [==============================] - 2s 852us/step - loss: 0.4921 - accuracy: 0.8241 - val_loss: 0.3733 - val_accuracy: 0.8646\nEpoch 2/10\n1719/1719 [==============================] - 1s 788us/step - loss: 0.3670 - accuracy: 0.8647 - val_loss: 0.3491 - val_accuracy: 0.8706\nEpoch 3/10\n1719/1719 [==============================] - 2s 891us/step - loss: 0.3282 - accuracy: 0.8789 - val_loss: 0.3108 - val_accuracy: 0.8846\nEpoch 4/10\n1719/1719 [==============================] - 1s 775us/step - loss: 0.3058 - accuracy: 0.8853 - val_loss: 0.3005 - val_accuracy: 0.8934\nEpoch 5/10\n1719/1719 [==============================] - 1s 822us/step - loss: 0.2847 - accuracy: 0.8929 - val_loss: 0.3024 - val_accuracy: 0.8876\nEpoch 6/10\n1719/1719 [==============================] - 1s 769us/step - loss: 0.2725 - accuracy: 0.8971 - val_loss: 0.3025 - val_accuracy: 0.8932\nEpoch 7/10\n1719/1719 [==============================] - 1s 799us/step - loss: 0.2566 - accuracy: 0.9029 - val_loss: 0.2946 - val_accuracy: 0.8966\nEpoch 8/10\n1719/1719 [==============================] - 1s 776us/step - loss: 0.2473 - accuracy: 0.9064 - val_loss: 0.3052 - val_accuracy: 0.8944\nEpoch 9/10\n1719/1719 [==============================] - 1s 784us/step - loss: 0.2394 - accuracy: 0.9109 - val_loss: 0.3212 - val_accuracy: 0.8896\nEpoch 10/10\n1719/1719 [==============================] - 1s 818us/step - loss: 0.2280 - accuracy: 0.9131 - val_loss: 0.3100 - val_accuracy: 0.8936\n\n\n\n\nCode\n\npd.DataFrame(history.history).plot()\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()"
  },
  {
    "objectID": "core/blogs/image_classification/image_classification.html#testing-the-model",
    "href": "core/blogs/image_classification/image_classification.html#testing-the-model",
    "title": "Image classification",
    "section": "Testing the model",
    "text": "Testing the model\n\n\nCode\ntest_loss, test_acc = model.evaluate(test_data, test_label)\n\n\n\n313/313 [==============================] - 0s 433us/step - loss: 55.4865 - accuracy: 0.8696"
  },
  {
    "objectID": "core/blog.html",
    "href": "core/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAI agent with Python and Gemini LLM\n\n\n\n\n\nBuilding AI agent using llm\n\n\n\n\n\nJan 13, 2025\n\n\nAakash Basnet\n\n\n\n\n\n\n\n\n\n\n\n\nRunning LLM locally\n\n\n\n\n\nSetting up open soure llm locally and builing simple application on top of it\n\n\n\n\n\nJan 12, 2025\n\n\nAakash Basnet\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression using DNN\n\n\n\n\n\n Linear regression of California housing dataset using deep neural networks\n\n\n\n\n\nMar 4, 2024\n\n\nAakash Basnet\n\n\n\n\n\n\n\n\n\n\n\n\nML OPS Ideas\n\n\n\n\n\nML ops idea and principles\n\n\n\n\n\nMar 4, 2024\n\n\nAakash Basnet\n\n\n\n\n\n\n\n\n\n\n\n\nImage classification\n\n\n\n\n\n image classification of Fashion MINST dataset\n\n\n\n\n\nFeb 19, 2024\n\n\nAakash Basnet\n\n\n\n\n\n\n\n\n\n\n\n\nWebscraping Indeed Job Portal\n\n\n\n\n\nwebscraping with python \n\n\n\n\n\nFeb 3, 2024\n\n\nAakash Basnet\n\n\n\n\n\n\n\n\n\n\n\n\nStock Market Analysis\n\n\n\n\n\nExtracting, Analysing and Predicting\n\n\n\n\n\nJan 15, 2024\n\n\nAakash Basnet\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "core/404.html",
    "href": "core/404.html",
    "title": "Page Not Found",
    "section": "",
    "text": "The page you requested cannot be found (perhaps it was moved or renamed).\nYou may want to try searching to find the page’s new location.\n\n\n\n Back to top"
  },
  {
    "objectID": "core/blogs/stock_market_analysis/stock_market_analysis.html",
    "href": "core/blogs/stock_market_analysis/stock_market_analysis.html",
    "title": "Stock Market Analysis",
    "section": "",
    "text": "“Stock Analysis(Generated by Imagen3)”\n\n\n\nInstallation\nOn your terminal run the following command to install yfinance module.\npip install yfinance\n\n\nExtract Company in SP500\nFirstly, lets extract the SP500 tickers from wikipedia table.\n\n\nCode\nimport pandas as pd\nimport requests\nimport yfinance as yf\n\nfrom pprint import pprint\n\nstart_date = '2020-01-01'\nend_date = '2025-01-15'\n\ntickers = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\ntickers['Symbol'] = tickers['Symbol'].apply(lambda x: x.strip())\ntickers.head()\n\n\n\n\n\n\n\n\n\nSymbol\nSecurity\nGICS Sector\nGICS Sub-Industry\nHeadquarters Location\nDate added\nCIK\nFounded\n\n\n\n\n0\nMMM\n3M\nIndustrials\nIndustrial Conglomerates\nSaint Paul, Minnesota\n1957-03-04\n66740\n1902\n\n\n1\nAOS\nA. O. Smith\nIndustrials\nBuilding Products\nMilwaukee, Wisconsin\n2017-07-26\n91142\n1916\n\n\n2\nABT\nAbbott Laboratories\nHealth Care\nHealth Care Equipment\nNorth Chicago, Illinois\n1957-03-04\n1800\n1888\n\n\n3\nABBV\nAbbVie\nHealth Care\nBiotechnology\nNorth Chicago, Illinois\n2012-12-31\n1551152\n2013 (1888)\n\n\n4\nACN\nAccenture\nInformation Technology\nIT Consulting & Other Services\nDublin, Ireland\n2011-07-06\n1467373\n1989\n\n\n\n\n\n\n\n\n\nDownload history data for SP500 companies\nFor each symbol in SP500, lets download daily historical data.\n\n\nCode\n\ndata = yf.download(tickers.Symbol.to_list(),\n                   start=start_date,\n                   end=end_date,\n                   interval='1D',\n                   auto_adjust=True)\ndata.tail()\n\n\n[*********************100%***********************]  503 of 503 completed\n\n2 Failed downloads:\n['BRK.B']: YFTzMissingError('$%ticker%: possibly delisted; no timezone found')\n['BF.B']: YFPricesMissingError('$%ticker%: possibly delisted; no price data found  (1d 2020-01-01 -&gt; 2025-01-15)')\n\n\n\n\n\n\n\n\nPrice\nAdj Close\nClose\n...\nVolume\n\n\nTicker\nBF.B\nBRK.B\nA\nAAPL\nABBV\nABNB\nABT\nACGL\nACN\nADBE\n...\nWTW\nWY\nWYNN\nXEL\nXOM\nXYL\nYUM\nZBH\nZBRA\nZTS\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2025-01-07\nNaN\nNaN\n137.410004\n242.210007\n179.529999\n131.289993\n113.400002\n92.250000\n356.390015\n422.630005\n...\n402500\n2588200\n2195300\n3017800\n12625900\n1232400\n2103000\n1649100\n353800\n2488500\n\n\n2025-01-08\nNaN\nNaN\n137.000000\n242.699997\n178.500000\n130.800003\n114.250000\n92.660004\n357.730011\n419.579987\n...\n655000\n3939100\n1862300\n3714700\n17858100\n1274500\n2025400\n2385600\n413600\n2353200\n\n\n2025-01-10\nNaN\nNaN\n137.470001\n236.850006\n175.169998\n129.630005\n112.309998\n90.169998\n349.790009\n405.920013\n...\n594700\n3529500\n2655900\n5441000\n19304500\n1334500\n2555000\n2709300\n460000\n3179500\n\n\n2025-01-13\nNaN\nNaN\n141.949997\n234.399994\n176.740005\n128.850006\n113.190002\n90.830002\n349.140015\n408.500000\n...\n459400\n4299500\n1850500\n3114600\n17073400\n1154300\n2163100\n1565800\n505200\n2306100\n\n\n2025-01-14\nNaN\nNaN\n143.429993\n233.279999\n175.550003\n127.599998\n113.019997\n91.989998\n348.989990\n412.709991\n...\n441700\n4151400\n2773800\n5952800\n11187700\n2137000\n1793600\n1457100\n414500\n3608200\n\n\n\n\n5 rows × 2517 columns\n\n\n\n\n\nCalculate Rolling Average\nNow, The 100 day rolling average is calculated and compared with latest closing price. Then, the data where the 100 days rolling average is less than latest closing price is filtered\n\n\nCode\nwindow_size = 100\nresults = []\nfor ticker in data.columns.get_level_values(1).unique():\n\n    ticker_data = data['Close'][ticker]\n    results.append((ticker,\n                    ticker_data.index[-1],\n                    ticker_data.iloc[-1].round(2),\n                   (ticker_data.tail(window_size).sum()/window_size).round(2))\n    )\n\nrolling_avg_df = pd.DataFrame(results, columns=['Ticker','Date','Close', f'{window_size}_MA_Close'])\nrolling_avg_df.dropna(inplace=True)\nprint(rolling_avg_df.shape)\nrolling_avg_df.head(10)\n\n\n(501, 4)\n\n\n\n\n\n\n\n\n\nTicker\nDate\nClose\n100_MA_Close\n\n\n\n\n2\nA\n2025-01-14\n143.43\n137.74\n\n\n3\nAAPL\n2025-01-14\n233.28\n232.84\n\n\n4\nABBV\n2025-01-14\n175.55\n185.97\n\n\n5\nABNB\n2025-01-14\n127.60\n130.46\n\n\n6\nABT\n2025-01-14\n113.02\n114.81\n\n\n7\nACGL\n2025-01-14\n91.99\n99.87\n\n\n8\nACN\n2025-01-14\n348.99\n353.79\n\n\n9\nADBE\n2025-01-14\n412.71\n504.11\n\n\n10\nADI\n2025-01-14\n214.65\n221.07\n\n\n11\nADM\n2025-01-14\n51.19\n55.17\n\n\n\n\n\n\n\n\n\nCode\nfiltered_df = rolling_avg_df[rolling_avg_df['Close'] &lt; rolling_avg_df['100_MA_Close']]\nprint(filtered_df.shape)\nfiltered_df.head(20)\n\n\n(301, 4)\n\n\n\n\n\n\n\n\n\nTicker\nDate\nClose\n100_MA_Close\n\n\n\n\n4\nABBV\n2025-01-14\n175.55\n185.97\n\n\n5\nABNB\n2025-01-14\n127.60\n130.46\n\n\n6\nABT\n2025-01-14\n113.02\n114.81\n\n\n7\nACGL\n2025-01-14\n91.99\n99.87\n\n\n8\nACN\n2025-01-14\n348.99\n353.79\n\n\n9\nADBE\n2025-01-14\n412.71\n504.11\n\n\n10\nADI\n2025-01-14\n214.65\n221.07\n\n\n11\nADM\n2025-01-14\n51.19\n55.17\n\n\n15\nAEP\n2025-01-14\n94.50\n97.22\n\n\n16\nAES\n2025-01-14\n11.85\n15.54\n\n\n17\nAFL\n2025-01-14\n103.65\n108.17\n\n\n18\nAIG\n2025-01-14\n72.59\n74.20\n\n\n20\nAJG\n2025-01-14\n289.11\n289.50\n\n\n21\nAKAM\n2025-01-14\n90.58\n98.02\n\n\n22\nALB\n2025-01-14\n92.63\n95.06\n\n\n23\nALGN\n2025-01-14\n210.46\n226.82\n\n\n24\nALL\n2025-01-14\n186.81\n191.05\n\n\n25\nALLE\n2025-01-14\n129.75\n139.76\n\n\n26\nAMAT\n2025-01-14\n173.65\n183.29\n\n\n27\nAMCR\n2025-01-14\n9.52\n10.46\n\n\n\n\n\n\n\n\n\nTop holding in SP500\n\n\nCode\nspy = yf.Ticker('SPY').funds_data\nspy.top_holdings\n\n\n\n\n\n\n\n\n\nName\nHolding Percent\n\n\nSymbol\n\n\n\n\n\n\nAAPL\nApple Inc\n0.075770\n\n\nNVDA\nNVIDIA Corp\n0.065938\n\n\nMSFT\nMicrosoft Corp\n0.062729\n\n\nAMZN\nAmazon.com Inc\n0.041097\n\n\nMETA\nMeta Platforms Inc Class A\n0.025549\n\n\nTSLA\nTesla Inc\n0.022575\n\n\nGOOGL\nAlphabet Inc Class A\n0.022140\n\n\nAVGO\nBroadcom Inc\n0.021675\n\n\nGOOG\nAlphabet Inc Class C\n0.018142\n\n\nBRK-B\nBerkshire Hathaway Inc Class B\n0.016631\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "core/blogs/indeed/indeed-job-scraper.html",
    "href": "core/blogs/indeed/indeed-job-scraper.html",
    "title": "Webscraping Indeed Job Portal",
    "section": "",
    "text": "After navigating the developer toolbar for Indeed job listing, I found the pattern in the url query for each job title search and location. We can use this info to build the url. The link printed from the code below will take you to the Indeed page having listing for python developer in Dalla, TX\n\n\nCode\nimport pandas as pd\nimport requests\nimport time\n\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\n\n\n\ndef url_builder(job_title, location, page_number=10 ):\n    job_title = \"+\".join(job_title.split(\" \"))\n    location = \"+\".join(location.split(\" \"))\n    base_url = \"https://www.indeed.com/jobs\"\n    query_str = f\"?q={job_title}&l={location}\"\n    url = f\"{base_url}{query_str}\"\n     \n    return url\n\nprint(url_builder(job_title=\"python developer\", location=\"Dallas, TX\"))\n\n\nhttps://www.indeed.com/jobs?q=python+developer&l=Dallas,+TX"
  },
  {
    "objectID": "core/blogs/indeed/indeed-job-scraper.html#building-url",
    "href": "core/blogs/indeed/indeed-job-scraper.html#building-url",
    "title": "Webscraping Indeed Job Portal",
    "section": "",
    "text": "After navigating the developer toolbar for Indeed job listing, I found the pattern in the url query for each job title search and location. We can use this info to build the url. The link printed from the code below will take you to the Indeed page having listing for python developer in Dalla, TX\n\n\nCode\nimport pandas as pd\nimport requests\nimport time\n\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\n\n\n\ndef url_builder(job_title, location, page_number=10 ):\n    job_title = \"+\".join(job_title.split(\" \"))\n    location = \"+\".join(location.split(\" \"))\n    base_url = \"https://www.indeed.com/jobs\"\n    query_str = f\"?q={job_title}&l={location}\"\n    url = f\"{base_url}{query_str}\"\n     \n    return url\n\nprint(url_builder(job_title=\"python developer\", location=\"Dallas, TX\"))\n\n\nhttps://www.indeed.com/jobs?q=python+developer&l=Dallas,+TX"
  },
  {
    "objectID": "core/blogs/indeed/indeed-job-scraper.html#scraping-the-indeep-page-with-selenium",
    "href": "core/blogs/indeed/indeed-job-scraper.html#scraping-the-indeep-page-with-selenium",
    "title": "Webscraping Indeed Job Portal",
    "section": "Scraping the indeep page with selenium",
    "text": "Scraping the indeep page with selenium\nThe script below scrapes the data for the given job title and location. It uses selenium web driver to automate the data scraping. The web driver clicks ‘next’ button on pagination until the end of the page.\n\n\nCode\n\n\ndef get_data(job_title, location):\n    \n\n    url = url_builder(job_title=job_title, location=location)\n    driver = webdriver.Chrome()\n    driver.get(url)\n    time.sleep(5)\n\n    jobs = []\n    has_next = True\n    count = 1\n    while has_next:\n        \n        time.sleep(10)\n        cards = driver.find_elements(By.CLASS_NAME,'cardOutline')\n        for card in cards:\n            job_title = card.find_element(By.CLASS_NAME,'jobTitle')\n            job_title_text = job_title.text\n            job_id = job_title.find_element(By.TAG_NAME, 'a').get_attribute('data-jk')\n            location = card.find_element(By.CLASS_NAME,'company_location').text\n            job_description = card.find_element(By.CLASS_NAME,'underShelfFooter').text\n            \n            try:\n                pay,*_metadata = card.find_element(By.CLASS_NAME,'heading6').text.split('\\n')\n            except Exception as e:\n                pay = 'NA'\n                _metadata = []\n        \n            \n            jobs.append({\n                'job_title':job_title_text,\n                'location': location,\n                'description': job_description,\n                'pay rate': pay,\n                'metadata': _metadata,\n                'job_id':job_id,\n                'job_url': f\"https://www.indeed.com/viewjob?jk={job_id}\",\n                \n            })\n\n        try:\n            driver.find_element(By.CSS_SELECTOR,\"[data-testid='pagination-page-next']\").click()\n            count += 1\n        except Exception as e:\n            print(f\"Ending at page {count}\")\n            has_next = False\n\n    driver.close()\n    return jobs\n\n\n\n\n\nCode\njob_data = get_data(job_title='python developer', location='Fort Worth,TX')\n\n\n\nEnding at page 2\n\n\n\n\nCode\n\ndf = pd.DataFrame(job_data)\ndf.head(40)\n\n\n\n\n\n\n\n\n\njob_title\nlocation\ndescription\npay rate\nmetadata\njob_id\njob_url\n\n\n\n\n0\nPython Programmer who enjoys helping people sm...\nLifecorp\\nArlington, TX\nPython Programmer who enjoys helping people sm...\n$70,000 - $100,000 a year\n[Full-time, Choose your own hours]\nff1fe7ff0d3d04ad\nhttps://www.indeed.com/viewjob?jk=ff1fe7ff0d3d...\n\n\n1\nSoftware Developer\nlead4ward\\nTexas\nAs part of a small, focused team, you’ll provi...\n$100,000 - $120,000 a year\n[Full-time, Monday to Friday, +1]\n6c7f467bada6cc22\nhttps://www.indeed.com/viewjob?jk=6c7f467bada6...\n\n\n2\nSoftware Developer\nDream Entertainment Labs\\nDallas, TX 75252\nProjects will center around family entertainme...\n$60,000 - $75,000 a year\n[Full-time]\nc32a429e080a48f0\nhttps://www.indeed.com/viewjob?jk=c32a429e080a...\n\n\n3\nPython Developer || Dallas, TX (Local only) ||...\nANB Sourcing LLC\\nDallas, TX 75201\nMid-level (5 or more years) in Python Developm...\n\n[]\nd5dfbda8e8a781b1\nhttps://www.indeed.com/viewjob?jk=d5dfbda8e8a7...\n\n\n4\nPython Developer\nE-Business International Inc\\nPlano, TX 75024\nCreate Golang based microservices and librarie...\n\n[]\na0caa2b4d714913d\nhttps://www.indeed.com/viewjob?jk=a0caa2b4d714...\n\n\n5\nPython FullStack Developer with Node\nInclusion Cloud\\nDallas, TX\nProvide technical guidance and support to juni...\n\n[]\nec46c96556d4c31d\nhttps://www.indeed.com/viewjob?jk=ec46c96556d4...\n\n\n6\nPython Developer _ (Local to Dallas, TX)- Onsi...\nANB Sourcing LLC\\nDallas, TX 75201\nDallas, TX (Onsite job)-- Local only.\\nEmploye...\n\n[]\nfbe285b925e08a10\nhttps://www.indeed.com/viewjob?jk=fbe285b925e0...\n\n\n7\nPython Developer\nInfoQuest Consulting Group Inc.\\nFort Worth, TX\nDuration & Type: 6 months Contract with a majo...\n\n[]\ne3232c051e279108\nhttps://www.indeed.com/viewjob?jk=e3232c051e27...\n\n\n8\nPython Developer\nTek Ninjas\\nFort Worth, TX 76120\nPosition: Python Developer Location: Fort Wort...\n\n[]\na002a17ed558d9a8\nhttps://www.indeed.com/viewjob?jk=a002a17ed558...\n\n\n9\nPython Developer\nQatalys Software Technologies\\nIrving, TX\nAnalyzes business and technical requirements t...\n\n[]\na2a8cbd917cbf446\nhttps://www.indeed.com/viewjob?jk=a2a8cbd917cb...\n\n\n10\nLead Software Engineer\nCapital One\\n3.9\\nPlano, TX 75023\nThese advancements will then allow high-qualit...\nPay information not provided\n[Full-time]\n92491b6cd1e2158f\nhttps://www.indeed.com/viewjob?jk=92491b6cd1e2...\n\n\n11\nPython Developer- $100-110K\nFults & Associates\\nGrapevine, TX 76051\nNeed someone who works a lot with Software.\\nM...\n\n[]\nb308d7d83046bef9\nhttps://www.indeed.com/viewjob?jk=b308d7d83046...\n\n\n12\nSr. Python Developer\nAmazee Global Ventures Inc\\nPlano, TX\nExperience in leading development teams and me...\n$55 - $60 an hour\n[Full-time, +1, Monday to Friday]\n85e34afe849476f4\nhttps://www.indeed.com/viewjob?jk=85e34afe8494...\n\n\n13\nJava Python Developer\nInfosys\\nRichardson, TX\nExperience in python development and libraries...\nPay information not provided\n[]\nfbf2ba53d3fc0f3e\nhttps://www.indeed.com/viewjob?jk=fbf2ba53d3fc...\n\n\n14\nPython Sr Developer\nInclusion Cloud\\nDallas, TX\nActive participation in agile (scrum) developm...\nPay information not provided\n[]\nb4fe0b9fcc1b77d5\nhttps://www.indeed.com/viewjob?jk=b4fe0b9fcc1b...\n\n\n\n\n\n\n\n\n\nCode\ndf.shape\n\n\n(15, 7)"
  },
  {
    "objectID": "core/blogs/indeed/indeed-job-scraper.html#rotating-proxies",
    "href": "core/blogs/indeed/indeed-job-scraper.html#rotating-proxies",
    "title": "Webscraping Indeed Job Portal",
    "section": "Rotating Proxies",
    "text": "Rotating Proxies\nThe proxies needs to be rotated to not be detected by anti scrapping tools used by the servers. For this we will scrape the list of free available ip address and test them using multithreading. This will filter the working proxies. Later on, we will use working proxies to make the request\n\n\nCode\n\n\n\ndef extract_proxies():\n    print(\"Extracting proxies...\")\n    proxy_url  = \"https://www.us-proxy.org/\"\n    r = requests.get(proxy_url)\n    dfs  = pd.read_html(r.text)\n    df = dfs[0]\n    print(df.shape)\n    return df\nproxies_df = extract_proxies()\nproxies_df.head(20)\n   \n\n\n/var/folders/22/2rvpv_m90c30mhtk77k1jd440000gn/T/ipykernel_39218/3797928275.py:1: DeprecationWarning: \nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\nbut was not found to be installed on your system.\nIf this would cause problems for you,\nplease provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n        \n  import pandas as pd\n\n\nExtracting proxies...\n(200, 8)\n\n\n/var/folders/22/2rvpv_m90c30mhtk77k1jd440000gn/T/ipykernel_39218/3797928275.py:9: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n  dfs  = pd.read_html(r.text)\n\n\n\n\n\n\n\n\n\nIP Address\nPort\nCode\nCountry\nAnonymity\nGoogle\nHttps\nLast Checked\n\n\n\n\n0\n104.225.220.233\n80\nUS\nUnited States\nelite proxy\nyes\nno\n3 secs ago\n\n\n1\n23.254.231.55\n80\nUS\nUnited States\nelite proxy\nyes\nno\n3 secs ago\n\n\n2\n50.217.226.42\n80\nUS\nUnited States\nanonymous\nno\nno\n4 secs ago\n\n\n3\n50.223.239.185\n80\nUS\nUnited States\nanonymous\nno\nno\n4 secs ago\n\n\n4\n50.174.145.15\n80\nUS\nUnited States\nanonymous\nno\nno\n4 secs ago\n\n\n5\n50.174.214.220\n80\nUS\nUnited States\nanonymous\nno\nno\n4 secs ago\n\n\n6\n50.217.226.46\n80\nUS\nUnited States\nanonymous\nno\nno\n4 secs ago\n\n\n7\n50.200.12.83\n80\nUS\nUnited States\nanonymous\nno\nno\n4 secs ago\n\n\n8\n50.168.72.118\n80\nUS\nUnited States\nanonymous\nno\nno\n4 secs ago\n\n\n9\n50.207.199.85\n80\nUS\nUnited States\nanonymous\nno\nno\n4 secs ago\n\n\n10\n50.174.214.219\n80\nUS\nUnited States\nanonymous\nno\nno\n4 secs ago\n\n\n11\n68.185.57.66\n80\nUS\nUnited States\nanonymous\nno\nno\n4 secs ago\n\n\n12\n50.221.74.130\n80\nUS\nUnited States\nanonymous\nno\nno\n4 secs ago\n\n\n13\n50.174.145.8\n80\nUS\nUnited States\nanonymous\nno\nno\n4 secs ago\n\n\n14\n50.173.140.151\n80\nUS\nUnited States\nanonymous\nno\nno\n4 secs ago\n\n\n15\n50.168.72.115\n80\nUS\nUnited States\nanonymous\nno\nno\n4 secs ago\n\n\n16\n50.170.90.27\n80\nUS\nUnited States\nanonymous\nno\nno\n4 secs ago\n\n\n17\n50.168.163.176\n80\nUS\nUnited States\nanonymous\nno\nno\n4 secs ago\n\n\n18\n50.223.246.226\n80\nUS\nUnited States\nanonymous\nno\nno\n4 secs ago\n\n\n19\n50.207.199.84\n80\nUS\nUnited States\nanonymous\nno\nno\n4 secs ago"
  },
  {
    "objectID": "core/blogs/building_ai_agent/building_ai_agent.html",
    "href": "core/blogs/building_ai_agent/building_ai_agent.html",
    "title": "AI agent with Python and Gemini LLM",
    "section": "",
    "text": "“AI Agents (Generated by Imagen3)”\n\n\n\nInstallation\nOn your terminal run the following command to install ollama module.\npip install ollama\n\nPull and run the model of you choice. For this tutorial we are running Gemma2 2B model, since it is small and powerful\nollama pull gemma2:2b\nInstall Langchain community module\npip install langchain-ollama\n\n\nLoading Model\nGemma2:2b is used to build the agent, as it is small is size and powerful too.\n\nfrom langchain_ollama.llms import OllamaLLM\nfrom langchain_core.prompts import ChatPromptTemplate\nimport json\nfrom pprint import pprint\n\nllm = OllamaLLM(model=\"gemma2:2b\",\n                temperature = 0)\n\n\n\nLets build LLM agent to translate one language to another.\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"\"\"\n            You are a resume generator.\n            You will generate the resume for a given user name and the job title.\n            You will print out the final resume that will be submitted to a job portal.\n            Make a four sections: Summary, Skills, Work Experience and Education.\n            Summary value is a strig.\n            Skill value is a dictionary object with section and list of skill. \n            Work experience value is a dictionary with work title and list of responsibilities.\n            There should be 10 responsibilities foe each work title.\n            Education value is a dictionary with degree level and information like graduated year and university name, location.\n            Provide all the output as json object only\n\n            \"\"\",\n        ),\n        (\n            \"human\",\n            \"\"\" \n            user name: {user_name},\n            job title: {job_title},\n            bachelors: {bachelor_education},\n            masters: {master_education},\n            work experiences: {work_experiences}\n            \"\"\"\n        ),\n    ]\n)\n\nchain = prompt | llm\n\n\n\nuser_data = {\n    \"user_name\": \"Jhon Doe\",\n    \"job_title\": \"Machine Learning Engineer\",\n    \"bachelor_education\": \"University of New Mexico,2018, Bsc in Computer Science and Information System\",\n    \"master_education\": \"Cumberlands University, 2023, Msc in Information Technology and System\",\n    \"work_experiences\": \"\"\" Fidelity, Machine learning Engineer|\n                            Bank of America, BigData/Machine learning Engineer|\n                            Penny Mac, Data Engineer|\n                            Vitol, Data Engineer|\n                            Bank of America, Backend Engineer|\n                            UNM BBER, Fullstack Developer\"\"\"\n}\nresult = \"\"\nasync for chunk in chain.astream(user_data):\n    result = result + chunk\n    print(chunk, end=\"\", flush=True)\n\n\n```json\n{\n  \"Summary\": \"Highly motivated and results-oriented Machine Learning Engineer with a proven track record of developing innovative solutions using advanced algorithms. Expertise in data analysis, machine learning model development, and implementation across diverse industries.\",\n  \"Skills\": {\n    \"Technical Skills\": [\n      \"Machine Learning\",\n      \"Deep Learning\",\n      \"Python\",\n      \"TensorFlow\",\n      \"Scikit-learn\",\n      \"Data Visualization\",\n      \"Cloud Computing (AWS)\",\n      \"Big Data Analytics\",\n      \"SQL\",\n      \"R\"\n    ],\n    \"Soft Skills\": [\n      \"Communication\",\n      \"Problem Solving\",\n      \"Teamwork\",\n      \"Adaptability\",\n      \"Critical Thinking\",\n      \"Time Management\"\n    ]\n  },\n  \"Work Experience\": {\n    \"Fidelity\": [\n      \"Developed and implemented machine learning models to predict customer churn, resulting in a 10% reduction in churn rate.\",\n      \"Designed and built data pipelines for real-time data analysis, improving operational efficiency by 20%.\",\n      \"Collaborated with cross-functional teams to develop and deploy AI-powered solutions for fraud detection.\",\n      \"Utilized Python libraries like Pandas and NumPy for data manipulation and statistical analysis.\",\n      \"Conducted A/B testing on machine learning models to optimize model performance and accuracy.\",\n      \"Presented findings and recommendations to stakeholders, effectively communicating complex technical concepts.\",\n      \"Mentored junior engineers on best practices in machine learning development.\",\n      \"Participated in code reviews and provided constructive feedback to improve team code quality.\"\n    ],\n    \"Bank of America\": [\n      \"Developed and implemented machine learning models for credit risk assessment, leading to a 5% improvement in loan approval rates.\",\n      \"Designed and built data pipelines for real-time fraud detection, reducing fraudulent transactions by 15%.\",\n      \"Collaborated with data scientists to develop predictive models for customer segmentation and targeting.\",\n      \"Utilized SQL and Python libraries like Pandas and NumPy for data analysis and model development.\",\n      \"Conducted A/B testing on machine learning models to optimize model performance and accuracy.\",\n      \"Presented findings and recommendations to stakeholders, effectively communicating complex technical concepts.\",\n      \"Mentored junior engineers on best practices in machine learning development.\",\n      \"Participated in code reviews and provided constructive feedback to improve team code quality.\"\n    ],\n    \"Penny Mac\": [\n      \"Designed and implemented data pipelines for real-time data analysis, improving operational efficiency by 20%.\",\n      \"Developed and deployed machine learning models for predictive maintenance, reducing equipment downtime by 10%.\",\n      \"Collaborated with engineers to develop and implement data visualization dashboards.\",\n      \"Utilized Python libraries like Pandas and NumPy for data manipulation and statistical analysis.\",\n      \"Conducted A/B testing on machine learning models to optimize model performance and accuracy.\",\n      \"Presented findings and recommendations to stakeholders, effectively communicating complex technical concepts.\",\n      \"Mentored junior engineers on best practices in data engineering.\",\n      \"Participated in code reviews and provided constructive feedback to improve team code quality.\"\n    ],\n    \"Vitol\": [\n      \"Developed and implemented machine learning models for demand forecasting, improving inventory management by 15%.\",\n      \"Designed and built data pipelines for real-time market analysis, enabling faster decision making.\",\n      \"Collaborated with engineers to develop and implement data visualization dashboards.\",\n      \"Utilized Python libraries like Pandas and NumPy for data manipulation and statistical analysis.\",\n      \"Conducted A/B testing on machine learning models to optimize model performance and accuracy.\",\n      \"Presented findings and recommendations to stakeholders, effectively communicating complex technical concepts.\",\n      \"Mentored junior engineers on best practices in machine learning development.\",\n      \"Participated in code reviews and provided constructive feedback to improve team code quality.\"\n    ],\n    \"Bank of America\": [\n      \"Developed and implemented backend systems for financial applications using Java and Spring Boot.\",\n      \"Designed and built RESTful APIs for data integration and communication.\",\n      \"Collaborated with developers to implement new features and functionalities.\",\n      \"Utilized SQL and NoSQL databases for data storage and retrieval.\",\n      \"Conducted code reviews and provided constructive feedback to improve team code quality.\",\n      \"Participated in technical design discussions and contributed to architectural decisions.\"\n    ],\n    \"UNM BBER\": [\n      \"Developed full-stack web applications using React, Node.js, and Express.js.\",\n      \"Designed and implemented user interfaces for various functionalities.\",\n      \"Collaborated with team members to develop and deploy new features.\",\n      \"Utilized version control systems like Git for code management.\",\n      \"Conducted unit testing and integration testing to ensure application functionality.\",\n      \"Participated in technical design discussions and contributed to architectural decisions.\"\n    ]\n  },\n  \"Education\": {\n    \"Bachelor's Degree\": {\n      \"University\": \"University of New Mexico\",\n      \"Degree\": \"Bsc in Computer Science and Information System\",\n      \"Graduation Year\": 2018\n    },\n    \"Master's Degree\": {\n      \"University\": \"Cumberlands University\",\n      \"Degree\": \"Msc in Information Technology and System\",\n      \"Graduation Year\": 2023\n    }\n  }\n}\n```\n\n\n\n# Replace and assign back to original content\nresult = result.replace(\"```json\", \"\")\nresult = result.replace(\"```\", \"\")\n\n# load the string output as json output\njson_result = json.loads(result)\n\npprint(json_result['Education'])\npprint(json_result[\"Skills\"])\n\n{\"Bachelor's Degree\": {'Degree': 'Bsc in Computer Science and Information '\n                                 'System',\n                       'Graduation Year': 2018,\n                       'University': 'University of New Mexico'},\n \"Master's Degree\": {'Degree': 'Msc in Information Technology and System',\n                     'Graduation Year': 2023,\n                     'University': 'Cumberlands University'}}\n{'Soft Skills': ['Communication',\n                 'Problem Solving',\n                 'Teamwork',\n                 'Adaptability',\n                 'Critical Thinking',\n                 'Time Management'],\n 'Technical Skills': ['Machine Learning',\n                      'Deep Learning',\n                      'Python',\n                      'TensorFlow',\n                      'Scikit-learn',\n                      'Data Visualization',\n                      'Cloud Computing (AWS)',\n                      'Big Data Analytics',\n                      'SQL',\n                      'R']}\n\n\n\n# TODO add task to llm agent\n\"\"\"\nStep 1: Scrape the job portal\nStep 2: For each job listing requirement and users resume, generate a resume in json format using LLM\nStep 3: Generate the Cover letter using LLM\nStep 3: Convert the resume in docs or pdf format with a decent design\nStep 4: Submit the job application or Send the resume and cover letter to company through email\n\"\"\"\n\n'\\nStep 1: Scrape the job portal\\nStep 2: For each job listing requirement and users resume, generate a resume in json format using LLM\\nStep 3: Generate the Cover letter using LLM\\nStep 3: Convert the resume in docs or pdf format with a decent design\\nStep 4: Submit the job application or Send the resume and cover letter to company through email\\n'\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "core/about.html",
    "href": "core/about.html",
    "title": "Aakash Basnet",
    "section": "",
    "text": "Greetings! I’m Aakash Basnet, a Software Engineer with 6 years of hands-on experience.\nMy journey involves working closely with clients as a consultant, where I’ve tackled various projects, from crafting web applications and dashboards to implementing machine learning solutions and web scraping tools. I take pride in simplifying complex tech challenges and creating solutions that align with clients’ unique needs.\nBeyond coding, I find joy in hiking, mountain biking, boxing and reading, maintaining a healthy work-life balance. Let’s connect and explore how we can create something exceptional together!\nSchedule time with Aakash\nDownload CV"
  },
  {
    "objectID": "core/about.html#education",
    "href": "core/about.html#education",
    "title": "Aakash Basnet",
    "section": "Education",
    "text": "Education\nUniversity of New Mexico, Bsc. in Computer Science| Dec 2018.\nCumberland University, Msc. in Information Technology | May 2023."
  },
  {
    "objectID": "core/about.html#skills",
    "href": "core/about.html#skills",
    "title": "Aakash Basnet",
    "section": "Skills",
    "text": "Skills\nProgramming language: Python, Javascript, HTML, CSS, Matlab, C, Haskell\nFrameworks: Flask, FastAPI,Django, Scrapy, beautifulsoup, selenium,Quarto, Airflow, Pyspark\nCI/CD : git, Docker\nDatabase: SQL, NoSQL, HDFS\nCloud: AWS( EC2, S3, RDS, Lamba, SQS, Sagemaker)\nTools: Nginx, gUnicorn, guvicorn, cornjob\nData analysis and Machine learning: Keras, Tensorfolw, pyTorch, scikit-learn, pandas, numpy, matplotlib"
  },
  {
    "objectID": "core/about.html#experience",
    "href": "core/about.html#experience",
    "title": "Aakash Basnet",
    "section": "Experience",
    "text": "Experience\nBank of America | Backend Big Data/ Machine learning Engineer | June 2022 - Jan 2024\nPennyMac | Backend/Machine Learning Engineer | August 2021 - May 2022\nVitol | Backend/ Machine Learning Engineer | April 2020 - July 2021\nBank of America | Full Stack Engineer | April 2019 - Dec 2019\nBBER | Full Stack Developer (internship) | May 2018 - Dec 2018\nCAPS| Programming/Math Tutor | Jan 2016 - June 2018"
  },
  {
    "objectID": "core/testimonials/1_Tyler_Ryan/tyler_ryan.html",
    "href": "core/testimonials/1_Tyler_Ryan/tyler_ryan.html",
    "title": "Tyler Ryan",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "core/contact.html",
    "href": "core/contact.html",
    "title": "Contact",
    "section": "",
    "text": "Thank you for your interest in reaching out to me.\nWhether you have queries, require consulting services, or are seeking solutions, I’m here to help. Your questions matter, and I’d love to receive your message.I am committed to providing prompt and personalized responses to address your specific needs.\nYou can use the contact form below to get in touch, or if you prefer, schedule a meeting  at your convenience.\n\n \n\nEmail *\n  \n\n\nSubject *\n  \n\n\nMessage *\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "core/solutions/4_Web Application/webapplication.html",
    "href": "core/solutions/4_Web Application/webapplication.html",
    "title": "Web Application",
    "section": "",
    "text": "Step into the future of online interaction with our Web Application services. We create user-friendly websites to make your business stand out.\n\n\nGet a website that suits your business perfectly. We use modern technology to build websites that work just right for you.\n\n\n\nWe cover everything your website needs: - Easy-to-use design. - Features that make your website work well. - Designs that look good on phones, tablets, and computers. - Behind-the-scenes technology to manage your data smoothly.\n\n\n\nWe use top-notch tools to make your website awesome: - Modern design tools like React, Angular, or Vue.js. - Strong backend tools like Node.js, Django, or Flask. - Different types of databases to store information. - Cloud services to make sure your website can handle lots of visitors.\n\n\n\nSee how our recent websites have made a difference: - Easy-to-use interfaces that people love. - Features that make websites work better. - Connecting everything behind the scenes to keep information organized.\n\n\n\nWhether you have a specific website idea or want to improve your online presence, let’s talk. We can make your online experience better!"
  },
  {
    "objectID": "core/solutions/4_Web Application/webapplication.html#personalized-web-solutions",
    "href": "core/solutions/4_Web Application/webapplication.html#personalized-web-solutions",
    "title": "Web Application",
    "section": "",
    "text": "Get a website that suits your business perfectly. We use modern technology to build websites that work just right for you."
  },
  {
    "objectID": "core/solutions/4_Web Application/webapplication.html#all-inclusive-website-development",
    "href": "core/solutions/4_Web Application/webapplication.html#all-inclusive-website-development",
    "title": "Web Application",
    "section": "",
    "text": "We cover everything your website needs: - Easy-to-use design. - Features that make your website work well. - Designs that look good on phones, tablets, and computers. - Behind-the-scenes technology to manage your data smoothly."
  },
  {
    "objectID": "core/solutions/4_Web Application/webapplication.html#our-tools",
    "href": "core/solutions/4_Web Application/webapplication.html#our-tools",
    "title": "Web Application",
    "section": "",
    "text": "We use top-notch tools to make your website awesome: - Modern design tools like React, Angular, or Vue.js. - Strong backend tools like Node.js, Django, or Flask. - Different types of databases to store information. - Cloud services to make sure your website can handle lots of visitors."
  },
  {
    "objectID": "core/solutions/4_Web Application/webapplication.html#recent-website-successes",
    "href": "core/solutions/4_Web Application/webapplication.html#recent-website-successes",
    "title": "Web Application",
    "section": "",
    "text": "See how our recent websites have made a difference: - Easy-to-use interfaces that people love. - Features that make websites work better. - Connecting everything behind the scenes to keep information organized."
  },
  {
    "objectID": "core/solutions/4_Web Application/webapplication.html#boost-your-online-presence",
    "href": "core/solutions/4_Web Application/webapplication.html#boost-your-online-presence",
    "title": "Web Application",
    "section": "",
    "text": "Whether you have a specific website idea or want to improve your online presence, let’s talk. We can make your online experience better!"
  },
  {
    "objectID": "core/solutions/5_Tutoring and Mentoring/tutoring_and_mentoring.html",
    "href": "core/solutions/5_Tutoring and Mentoring/tutoring_and_mentoring.html",
    "title": "Tutoring",
    "section": "",
    "text": "Enhance your proficiency in Python, programming, software engineering, and AI through our tutoring and mentoring service. We offer tailored guidance to individuals and foster collaborative learning in group sessions, ensuring a well-rounded educational experience.\n\n\nGet a tutoring experience that’s just right for you. We provide one-on-one sessions for personalized attention, focusing on Python, programming, software engineering, and AI to meet your specific learning goals.\n\n\n\nJoin our group sessions for a collaborative learning environment. Engage with peers as we explore a range of topics together, covering Python basics, programming languages like Java or JavaScript, software engineering principles, and the fundamentals of Artificial Intelligence.\n\n\n\nExplore a variety of topics, including: - Python basics and advanced concepts. - Programming languages such as Java, C++, or JavaScript. - Software engineering principles and best practices. - Introduction to Artificial Intelligence and machine learning.\n\n\n\nWe use a friendly and approachable style to teach: - Step-by-step learning for ease of understanding. - Practical examples to apply what you learn. - Guidance in building real-world projects.\n\n\n\nDiscover how our tutor"
  },
  {
    "objectID": "core/solutions/5_Tutoring and Mentoring/tutoring_and_mentoring.html#personalized-learning-experience",
    "href": "core/solutions/5_Tutoring and Mentoring/tutoring_and_mentoring.html#personalized-learning-experience",
    "title": "Tutoring",
    "section": "",
    "text": "Get a tutoring experience that’s just right for you. We provide one-on-one sessions for personalized attention, focusing on Python, programming, software engineering, and AI to meet your specific learning goals."
  },
  {
    "objectID": "core/solutions/5_Tutoring and Mentoring/tutoring_and_mentoring.html#group-learning-atmosphere",
    "href": "core/solutions/5_Tutoring and Mentoring/tutoring_and_mentoring.html#group-learning-atmosphere",
    "title": "Tutoring",
    "section": "",
    "text": "Join our group sessions for a collaborative learning environment. Engage with peers as we explore a range of topics together, covering Python basics, programming languages like Java or JavaScript, software engineering principles, and the fundamentals of Artificial Intelligence."
  },
  {
    "objectID": "core/solutions/5_Tutoring and Mentoring/tutoring_and_mentoring.html#what-we-cover",
    "href": "core/solutions/5_Tutoring and Mentoring/tutoring_and_mentoring.html#what-we-cover",
    "title": "Tutoring",
    "section": "",
    "text": "Explore a variety of topics, including: - Python basics and advanced concepts. - Programming languages such as Java, C++, or JavaScript. - Software engineering principles and best practices. - Introduction to Artificial Intelligence and machine learning."
  },
  {
    "objectID": "core/solutions/5_Tutoring and Mentoring/tutoring_and_mentoring.html#our-approach",
    "href": "core/solutions/5_Tutoring and Mentoring/tutoring_and_mentoring.html#our-approach",
    "title": "Tutoring",
    "section": "",
    "text": "We use a friendly and approachable style to teach: - Step-by-step learning for ease of understanding. - Practical examples to apply what you learn. - Guidance in building real-world projects."
  },
  {
    "objectID": "core/solutions/5_Tutoring and Mentoring/tutoring_and_mentoring.html#recent-success-stories",
    "href": "core/solutions/5_Tutoring and Mentoring/tutoring_and_mentoring.html#recent-success-stories",
    "title": "Tutoring",
    "section": "",
    "text": "Discover how our tutor"
  },
  {
    "objectID": "core/solutions/3_Webscraping/webscraping.html",
    "href": "core/solutions/3_Webscraping/webscraping.html",
    "title": "Web Scraping",
    "section": "",
    "text": "Explore our comprehensive suite of services, offering web scraping, data mining, and data extraction solutions tailored for lead generation, business process automation, research, and marketing initiatives.\n\n\nOur web scrapers, written in Python (utilizing BeautifulSoup, Requests, Selenium), ensure precise data extraction, filtration, and packaging in versatile formats, including CSV, JSON, and XML.\n\n\n\n\nExtract data tables, text, images, and links, among other elements.\nFilter and compile data into various formats: JSON, XML, CSV, and SQL.\nSet up alerts for new content discovery.\nCapture screenshots or download full HTML of websites.\nConduct “real” interactions with websites, including clicking buttons, accessing drop-downs, and entering text into forms.\n\n\n\n\nUtilizing Python BeautifulSoup, Requests, Selenium, and Headless Chrome.\n\n\n\n\nSystem to rename thousands of images files stored in Dropbox using Python and Dropbox API.\nWeb app to translate Excel documents with Python Flask and Google Cloud Translation API.\nScraped a list of 100,000 funeral homes across the U.S.\nScraped a list of 3,000 martial arts institutes in the UK, identifying site tools, mobile responsiveness, and page load speed.\nAutomated scraping of property listings and publishing to a WordPress site.\nScraped product listings from a home appliances vendor using Selenium.\nDeveloped a system to scrape job listings from Indeed and import into a WordPress-based website.\n\n\n\n\nDo you have a web scraping, data extraction, or business process automation requirement? Let’s discuss how we can elevate your operations."
  },
  {
    "objectID": "core/solutions/3_Webscraping/webscraping.html#custom-web-scrapers",
    "href": "core/solutions/3_Webscraping/webscraping.html#custom-web-scrapers",
    "title": "Web Scraping",
    "section": "",
    "text": "Our web scrapers, written in Python (utilizing BeautifulSoup, Requests, Selenium), ensure precise data extraction, filtration, and packaging in versatile formats, including CSV, JSON, and XML."
  },
  {
    "objectID": "core/solutions/3_Webscraping/webscraping.html#web-scraping-features",
    "href": "core/solutions/3_Webscraping/webscraping.html#web-scraping-features",
    "title": "Web Scraping",
    "section": "",
    "text": "Extract data tables, text, images, and links, among other elements.\nFilter and compile data into various formats: JSON, XML, CSV, and SQL.\nSet up alerts for new content discovery.\nCapture screenshots or download full HTML of websites.\nConduct “real” interactions with websites, including clicking buttons, accessing drop-downs, and entering text into forms."
  },
  {
    "objectID": "core/solutions/3_Webscraping/webscraping.html#scraping-tools",
    "href": "core/solutions/3_Webscraping/webscraping.html#scraping-tools",
    "title": "Web Scraping",
    "section": "",
    "text": "Utilizing Python BeautifulSoup, Requests, Selenium, and Headless Chrome."
  },
  {
    "objectID": "core/solutions/3_Webscraping/webscraping.html#recent-projects",
    "href": "core/solutions/3_Webscraping/webscraping.html#recent-projects",
    "title": "Web Scraping",
    "section": "",
    "text": "System to rename thousands of images files stored in Dropbox using Python and Dropbox API.\nWeb app to translate Excel documents with Python Flask and Google Cloud Translation API.\nScraped a list of 100,000 funeral homes across the U.S.\nScraped a list of 3,000 martial arts institutes in the UK, identifying site tools, mobile responsiveness, and page load speed.\nAutomated scraping of property listings and publishing to a WordPress site.\nScraped product listings from a home appliances vendor using Selenium.\nDeveloped a system to scrape job listings from Indeed and import into a WordPress-based website."
  },
  {
    "objectID": "core/solutions/3_Webscraping/webscraping.html#get-in-touch",
    "href": "core/solutions/3_Webscraping/webscraping.html#get-in-touch",
    "title": "Web Scraping",
    "section": "",
    "text": "Do you have a web scraping, data extraction, or business process automation requirement? Let’s discuss how we can elevate your operations."
  }
]